<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Universe OpenAstronomy (Posts about stingray)</title><link>http://openastronomy.org/Universe_OA/</link><description></description><atom:link href="http://openastronomy.org/Universe_OA/categories/stingray.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><lastBuildDate>Mon, 08 Aug 2022 16:25:14 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>GSoC Blog#2</title><link>http://openastronomy.org/Universe_OA/posts/2022/07/20220726_0559_aman-pandey-afk/</link><dc:creator>AMAN PANDEY</dc:creator><description>&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/1*dfjiDfMcY8h9eEOySAnxbA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;The first half of the coding period is almost done, and here I am with the updates! As stated at the end of the last blog, I started the 3rd week by improving the &lt;a href="https://github.com/StingraySoftware/Stingray.jl/pull/2"&gt;second PR&lt;/a&gt;. I performed pretty intensive memory and performance analysis on the functions in fourier.jl, using BenchmarkTools and .mem files to analyze bottlenecks in the program. With my mentors’ help, I removed many allocation and type-stability related issues during that time. I also had some problems with non-idiomatic code, like I could use multiple-dispatch or dot broadcast in functions, some of which I solved, and some are due for refactoring after the mid-evaluation. After finishing the improvements and getting my second PR merged, I planned to work on LightCurves and implement periodograms and other APIs so that users can easily manipulate photon count data. But my mentor suggested I implement file reading and mechanisms to manage the GTIs (Good Time Intervals) obtained from these files.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Working out the GTI mechanisms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I had learned a lot from the previous PR, and one of the things was I should try to implement some of the methods in my way rather than using the python algorithms with an idiomatic Julian code in mind. I started the 4th week with methods for reading GTIs from a FITS file (For those unfamiliar, it is a file format for storing, processing, and transmitting scientific data, especially images). I used the FITSIO.jl package, and experimenting on the terminal led me to manipulate HDUs and their data. One thing I thoroughly thought of was the appropriate data structure of the GTIs. Should I use Intervals from the Intervals.jl package? Or a vector of vector like the python library does? I finally decided to use what I was using in the fourier.jl, an AbstractMatrix of Reals. It was easy to access data from them, and you could use slices to get a list of start and end times. For the operations, I had to convert among matrix, intervals frequently, and vectors as the Intervals.jl provided many ways to manipulate GTIs like union or intersection or getting its complement, i.e., Bad Time Intervals. With some more methods like creating masks and GTIs from conditions and implementing Tests for all of these functions, I was finished with the gti.jl file. A little performance analysis told me that the code was efficient. After some refactoring, like removing code duplication by merging union and intersection in the same function, I was done with the PR. It has some minor changes currently to be made before it gets merged.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;&lt;strong&gt;Working on the documentation&lt;/strong&gt;&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*lbM1DS0_-Ja0lb53Owb6PQ.png"&gt;&lt;/figure&gt;&lt;p&gt;As the mid evaluations have begun this week, I will be taking some time to write the docstring and deploy them on GitHub. The stingray python docs are pretty good, so I will mostly use them as a Base and tweak them as necessary (when there is a different function signature or the Julian way of doing things is different). Documenter.jl will be the package I would be using for this. The complete documentation will be a milestone for the end of the program, where I, along with my mentors, will try to write notebook tutorials for the package and host them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For the second phase&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I hope to qualify for the mid-evals, after which the second phase will begin. The core implementation is done; users can now use the library and its different methods to read from a file, process the data, and create periodograms. The main motive now will be to ease its access and implement other helpful features, a major one of which will be plotting the periodograms. I initially proposed working with these APIs before mid-evals, but who knows about the future? I have done some things meant for the other half, so I guess it’s okay. As an end note, I am pretty much excited for the other half of this program, it has made me learn a lot, and the workings of Julia awe-inspire me as I explore it side by side. Goodbye for now!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=63d283e1a60b" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2022/07/20220726_0559_aman-pandey-afk/</guid><pubDate>Tue, 26 Jul 2022 04:59:13 GMT</pubDate></item><item><title>GSoC @ Stingray: Diving into coding period. blog #2</title><link>http://openastronomy.org/Universe_OA/posts/2022/07/20220724_1811_mihirtripathi97/</link><dc:creator>Mihirtripathi</dc:creator><description>&lt;p&gt;Hey there!&lt;/p&gt;
&lt;p&gt;It is time to write a blog about my experience in the coding period so far. So here it goes.&lt;/p&gt;
&lt;p&gt;The official coding period for GSoC’22 started on the 13th of June. After discussing with mentors I decided that the first task would be to create a base structure of the code for Bexvar. As the method was already implemented by Dr. Johannes Buchner (who also suggested implementing this method in Stingray)and David Bogensberger, we decided to use this &lt;a href="https://github.com/JohannesBuchner/bexvar"&gt;implementation&lt;/a&gt; as a reference.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;I spent the first 2–3 days planning the structure of the code and planning its implementation in Stingray’s code base. At first, the implementation seemed easy. Although as I explored more and discussed my ideas with my mentors I realized that there are many small but important changes that need to be made.&lt;/p&gt;
&lt;p&gt;In addition to these, I realized that I had to set up a proper environment on my computer to write, edit and test the codes that I write. I decided to use &lt;a href="https://code.visualstudio.com/"&gt;Visual Studio Code&lt;/a&gt; (VS Code) for it. Before the contribution period, I had only a little familiarity with Github and had never used git in my local system. I somehow made it through the contribution period without installing it. Now since I had an open source project at hand which will require frequent commits, I decided to install git and GitHub desktop on my computer and learn to use git with CLI. I found out that along with many useful tools for professional programming, VS Code also provided support to use git inside from the editor, this seemed quite useful for my purpose. I learned how to install and work with multiple versions of Python. I learned how to set up a Python virtual environment and use it to test codes in a controlled environment. I had to go through multiple iterations of installing, uninstalling, and then reinstalling several packages to set everything perfectly. This made me a bit irritated but later on, I realized that this helped me save a lot of time while coding for the project.&lt;/p&gt;
&lt;p&gt;After going through a few iterations the base code was ready and working. It still needed some improvements and structural changes. It had a core function that reads the light curve data from an AstroPy table. After that, it would call internal functions successively to finally obtain the Bayesian Excess Variance of count rate. My mentor Prof. Matteo suggested that I start a PR with this version of the code. As I made this &lt;a href="https://github.com/StingraySoftware/stingray/pull/664"&gt;PR&lt;/a&gt;, we started discussing how to improve the code.&lt;/p&gt;
&lt;p&gt;My mentors pointed out that my core function &lt;em&gt;bexvar()&lt;/em&gt; needed further modularization. We decided to separate it into two functions, A core function &lt;em&gt;bexvar()&lt;/em&gt; which will accept light curve data as a set of NumPy arrays or lists and calculate bexvar, and a &lt;em&gt;bexvar_from_table()&lt;/em&gt; function which can read light curve data from an AstroPy table and call &lt;em&gt;bexvar()&lt;/em&gt;. This change made the core &lt;em&gt;bexvar()&lt;/em&gt; function more general as now users would not need to have light curve data in form of an AstroPy Table object to obtain bexvar. Also, in the future, we can create a function like &lt;em&gt;bexvar_from_table()&lt;/em&gt; which can read data from Stingray’s Lightcurve object and call &lt;em&gt;bexvar()&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;In the next phase, I updated existing docstrings, added docstrings where it was missing, and worked on code formating. At this stage, the code was still failing in Stingray’s CI tests. The reason behind the failures was that the code uses an integrator &lt;a href="https://johannesbuchner.github.io/UltraNest/_modules/ultranest/integrator.html#ReactiveNestedSampler"&gt;&lt;strong&gt;ReactiveNestedSampler&lt;/strong&gt;&lt;/a&gt; from &lt;a href="https://arxiv.org/abs/2101.09604"&gt;UltraNest — a robust, general purpose Bayesian inference engine&lt;/a&gt;. UltraNest is not a required dependency for Stingray, hence CI tests were failing with import error. To resolve this, the import statement of UltraNest in bexvar.py needed to be moved inside a try-except statement. This would restrict the user from using bexvar.py if UltraNest is not installed. With this change, the code has passed the majority of CI tests.&lt;/p&gt;
&lt;p&gt;The overall experience of the coding period so far has been good. During this period I learned a lot about the essentials of open-source programming. I am learning to think like a programmer. I learned how small improvements in code make it more general and versatile. I learned about the best practices to write code. I must say that all this time my mentors Matteo and Daniela have been quite supportive and helpful. They have patiently listened to my queries and doubts and have always helped me with them.&lt;/p&gt;
&lt;p&gt;My code is running perfectly now, from this week onwards I will be focussing on writing tests for the code. Writing tests for a code is a completely new experience for me. I am looking forward to learn a lot about professional code testing in the following weeks. I will soon be back with a blog on my experience with testing. Till then take care! Goodbye!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=f24a03c00014" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2022/07/20220724_1811_mihirtripathi97/</guid><pubDate>Sun, 24 Jul 2022 17:11:57 GMT</pubDate></item><item><title>GSoC @ Stingray: Begining of the journey. blog #1</title><link>http://openastronomy.org/Universe_OA/posts/2022/07/20220704_0357_mihirtripathi97/</link><dc:creator>Mihirtripathi</dc:creator><description>&lt;p&gt;Hey there! It has been about 15 days since I had written my last blog. It is time for the next one. In this blog, I will write about my experience in GSoC so far. I want to tell you about how I selected my project, how I prepared my project proposal, and what did I do after getting selected.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How did I select the organization?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, let’s start from the beginning! I started considering to apply in GSoC in late February. I first came to know about GSoC just a few months back. My knowledge about GSoC and open-source was quite limited. One of the first things that I did was to go through the &lt;a href="https://summerofcode.withgoogle.com/"&gt;GSoC website&lt;/a&gt; and learn more about the program. Being a beginner in the open-source community and also coming from a non-CS background I had no idea which organizations take part in GSoC and which of them would be most suitable for me. At this time, the mentoring organizations for GSoC”22 were not announced. I decided to check out mentoring organizations from past programs to get an idea. I wanted to work on a project that involve Physics or Astrophysics, and to my surprise, several such organizations did take part in previous programs.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;From this search, I got to know about &lt;a href="https://openastronomy.org/"&gt;OpenAstronomy&lt;/a&gt;. It is an umbrella organization of several open-source astronomy and astrophysics projects. From their &lt;a href="https://openastronomy.org/gsoc/"&gt;GSoC webpage&lt;/a&gt;, I came to know that they are applying to participate in GSoC”22. The webpage also contained guidelines for contributors, a list of project ideas for this year and for past years, and information about how to get in touch with mentors and communities of various member organizations. All these were quite informative and useful. I also checked out several other organizations as well. All of them had such information available online in various forms. A few days later the participating organizations were announced by GSoC and OpenAstronomy was one of them. By this time, as directed in contributor guidelines of both &lt;a href="https://google.github.io/gsocguides/student/"&gt;GSoC&lt;/a&gt; and &lt;a href="https://openastronomy.org/gsoc/student_guidelines.html"&gt;OpenAstronomy&lt;/a&gt;, I had introduced myself in chat channels and mailing lists of some organizations. I went through various project ideas proposed by these organizations and tried to find a project that aligned most with my interests. Being an astronomer, I wanted to work on an OpenAstronomy project and from the list of project ideas I liked the project called ‘Bexvar in Stingray’. So I decided to prepare a proposal for this project.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;About the project&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Let me tell you in brief about &lt;a href="https://docs.stingray.science/index.html"&gt;Stingray&lt;/a&gt; and ‘Bexvar’ first. Stingray is an open-source package for x-ray spectral timing written in python. It provides a number of tools to perform times series analysis and related tasks on astronomical light curves. In the case of faint x-ray sources, observed near the detection limits of telescopes, uncertainties in their light curves are dominated by Poissonian uncertainties. The Bayesian excess variance (Bexvar) method is useful to find variability intrinsic to sources from gappy light curves dominated by Poissonian uncertainties. It was developed by &lt;a href="https://arxiv.org/abs/2106.14529"&gt;Buchner et al. (2021)&lt;/a&gt;. The aim of the ‘Bexvar in Stingray’ project is to implement this method in Stingray, create test cases for the method based on real observational data and write appropriate documentation. I will write about the project in more detail in my next blog.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How did I make the proposal?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When I read about the project, my first instinct was to read the article by &lt;a href="https://arxiv.org/abs/2106.14529"&gt;Buchner et al. (2021)&lt;/a&gt; to know more about the method. After reading it I was convinced that I do want to work on implementing this method in Stingray. I started by introducing myself to the Stingray community through their slack channel. As per the &lt;a href="https://openastronomy.org/gsoc/student_guidelines.html"&gt;guideline&lt;/a&gt; by OpenAstronmy, one of the first things to do then was to make a PR in Stingray for the contribution period. For this, I went to the &lt;a href="https://github.com/StingraySoftware/stingray"&gt;GitHub repository&lt;/a&gt; of the core Stingray package. I went through the list of open issues and found issues labeled ‘good first issue’ and ‘GSoC’. I selected an issue that I thought was closely related to my project. The &lt;a href="https://github.com/StingraySoftware/stingray/issues/592"&gt;issue&lt;/a&gt; was about a bug in the CrossCorrelation class in stingray. The CrossCorrelation class of stingray has an internal method called time_shift. This provided the timeshift required between two lightcurves to get maximum cross-correlation between them. It was found that if two identical lightcurves are used then, the time_shift method should give zero as the answer irrespective of the length of the lightcurve. As the light curve will have maximum crosscorrelation with itself. However, the method did not give zero as an answer in the case of the light curve having an even number of time bins. I tried to solve this issue by changing how time_shift was calculated in the code. During this time I interacted with Prof. Matteo Bachetti, who is one of the mentors for the project. Before applying for GSoC, I had negligible experience in contributing to open-source. Matteo guided me a lot during this phase and finally, my &lt;a href="https://github.com/StingraySoftware/stingray/pull/647"&gt;PR&lt;/a&gt; was merged. To be honest this made me very happy! It was the first time when I actually made some contributions to open-source software. The next task was to prepare a proposal for the project. This was a long and tough process. I spent most of my time reading about the Bexvar method, trying to understand it, and figuring out how to write a proposal for its implementation. During this time I sent my draft proposals to Matteo, he guided me to make them better, I worked on his suggestion. This interaction also gave me a lot of insight into what is expected from a contributor. This entire experience of preparing a proposal for the project and taking suggestions from mentors was quite new to me, and I am really grateful that I had a chance to go through it.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/1*YFXjL3Wc5EqBE9vaXPHNBQ.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Community bonding period&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After a month of submitting a proposal, the selected proposals were announced. To my surprise, my proposal was selected. I was over the moon, I was happy, I was excited and also a little bit worried. With the selection, the three-week-long community bonding period started. This period is dedicated for selected contributors to bond with community members, and mentors, interact with them and get themselves familiar with the codebase. This acts as a warm-up period before the actual coding period starts. The first important thing that happened during this period was the project kickoff meeting with my mentors Matteo and Daniela. This was the first time when I virtually met both of them. We discussed a bit about the project, our background, and current work, our expectations from each other, etc. We decided to meet every week virtually and stay in contact during the week via slack. This was a truly nice experience for me, both of them are quite supportive, they explained things to me nicely and most importantly made me feel welcome in the community and made sure that I was comfortable. In the following weeks, I spent my time studying the implementation of &lt;a href="https://github.com/JohannesBuchner/bexvar"&gt;Bexvar&lt;/a&gt; by Buchner et al. (2021), learning about Bayesian statistics, and also understanding Stingray’s code base. I also learned more about testing in python and by the end of the community bonding period, I was ready to start coding. I planned to get a basic model of the method ready in the first few weeks after the community bonding period. Overall the community bonding period was great, It really helped me get into the right mindset for the coding period and be prepared for it. I learned a lot about Stingray, Bayesian statistics, and my project during this time.&lt;/p&gt;
&lt;p&gt;A lot more is yet to be covered. Right now I am on the 3 rd week of the coding period. I will soon write about my project and my work in the coding period in detail in my next blog. It is time to wrap this one up for now. See you soon!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=7ed5a47bec65" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2022/07/20220704_0357_mihirtripathi97/</guid><pubDate>Mon, 04 Jul 2022 02:57:22 GMT</pubDate></item><item><title>GSoC Blog #1</title><link>http://openastronomy.org/Universe_OA/posts/2022/07/20220702_0631_aman-pandey-afk/</link><dc:creator>AMAN PANDEY</dc:creator><description>&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/0*kmysU0LzpIzm8qV4.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;Its time for my first blog during the coding period! We are into our 3rd week in the coding period, and already the journey is getting exciting, a little challenging, of course, but I am enjoying it. As I had mentioned in the earlier blog, I started the initial week with implementing the helper functions, Fourier analysis, and tests related to them from Python to Julia. By god, it was not an easy way through.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dealing with tests&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I started by understanding what the functions inside fourier.py did and quickly realized that without running tests, I would have a hard time working out the insides of some of the methods. I thus, started writing functions is Julia, understanding the parameters and their types and using external packages like ResumableFunctions or Statistics wherever necessary. After implementing normalizations for arrays, I worked on tests where the problems began. Out of Index errors, types not supported, and the same functionality implemented differently in Julia and Python started to take most of my time.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;p&gt;For example, to create a histogram, we use &lt;em&gt;NumPy.histogram, &lt;/em&gt;pass it an array and number of bins, and it returns the required histogram in a tuple. For Julia, I had to use the package StatsBase, then call fit(Histogram, array, bins), which generates an object whose weight property is my result. But that is the fun. I’m slowly adjusting to handling such codes without class methods, which will be very necessary for this and the next week, as I look forward to implementing LightCurves and EventLists classes of the python package.&lt;/p&gt;
&lt;p&gt;Data handling, which I thought initially to be challenging, was although a smooth experience. Using the DataFrame package with the MetaData package turned out to be a very flexible approach while working with tables. I quickly implemented a large sample data using the HDF5 format suggested by my mentor.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Making the PR&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I was about to complete this work when the government made the internet in my area inaccessible due to local protests. I couldn’t work that efficiently for three days and definitely couldn’t communicate with my mentor. After the internet was back, I quickly sorted out some issues and made a PR, but it wasn’t too good to be merged due to a few reasons. I didn’t provide type-annotations on the functions as many arguments were nothing (null in Julia), and providing types on them would cause the function to produce an error. My code also had some memory allocation and performance issues. The most encouraging thing was how my mentor Mosè cooperated with these issues. He advised me on many things, provided me with quick fixes, and commented on learning from these mistakes. I am certainly looking to fix these problems as soon as possible, learning along to write practical and high-performance Julia code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The problem now&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I quickly changed the code to be type annotated as I already knew about the types while understanding the codebase. There were not many type instabilities as I already had checked my functions with @code_warntype macro, and it was mostly clean. The main problem I’m facing now is unlike Python, Julia doesn’t have a tradition of defaulting values with null and checking if it’s null to provide more functionality ahead in code. This is the result of one of my @code_warntype runs:&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*nVtVLI6OIUyMQ9TqlIbqtQ.png"&gt;&lt;figcaption&gt;The Yellow Colour Indicates a type-instability, though it maybe important to use Unions of Nothing and a Concrete Datatype, thus it is only a warning (not red)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The code is not entirely Julian, and the tests take slightly longer to run compared to Python.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/840/1*dJ7COYI3hPgd-O3OYiv65w.png"&gt;&lt;figcaption&gt;Python tests run in 3.9s on avg&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;Next two weeks&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I intend to implement creating and working with LightCurves and then move on to EventLists and implement related helper functions and filters. My approach now will be to understand and sketch the functioning with a thorough working of tests and then implement it from scratch in a Julian way. Alongside, I will be improving the fourier.jl functions with what I learn and hope to get the PR merged by the start of the following week. I will be back with some more accounts of this exciting journey!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=cc0c0995d56e" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2022/07/20220702_0631_aman-pandey-afk/</guid><pubDate>Sat, 02 Jul 2022 05:31:49 GMT</pubDate></item><item><title>GSoC @ Stingray blog #0</title><link>http://openastronomy.org/Universe_OA/posts/2022/06/20220616_0650_mihirtripathi97/</link><dc:creator>Mihirtripathi</dc:creator><description>&lt;p&gt;Hey there, welcome to this blog. My name is Mihir and I am a contributor for &lt;strong&gt;Google Summer of Code 2022&lt;/strong&gt;. This blog is the first in a series of blogs where I will write about my journey through the GSoC working for &lt;a href="https://stingray.science/"&gt;&lt;strong&gt;Stingray&lt;/strong&gt;&lt;/a&gt; under the umbrella organization&lt;strong&gt; &lt;/strong&gt;&lt;a href="https://openastronomy.org/"&gt;&lt;strong&gt;OpenAstronomy&lt;/strong&gt;&lt;/a&gt;. During the period of the next 12 weeks or so, I will be working toward implementing the Bayesian Excess Variance method in Stingray with help of my mentors Matteo and Daniela.&lt;/p&gt;
&lt;p&gt;If you are here then you might already know about the Google Summer of Code. If you do not then here is a brief description of it. It is a program, organized and conducted by Google annually. It starts somewhere around the winter equinox and ends sometime around the autumn equinox, i.e. throughout the summer. The program aims to bring new contributors to open-source software development. Of course, you can get more information about it by visiting its &lt;a href="https://summerofcode.withgoogle.com/"&gt;website&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Since this is my first blog, I will keep things simple and just write briefly about myself. By profession, I am a researcher and an educator. I did my post-graduation in Physics from the Tata Institute of Fundamental Research (TIFR), Mumbai. My research work is in the field of Star and exoplanet formation. I study the evolution and dynamics of circumstellar disks that form around young stars. Apart from this, I teach physics to undergrad students. I like to slow travel and sometimes I volunteer as a teacher and an artist. I like to read fiction and watch movies. Currently, I am staying in a small village, located amid the snow-covered mountains of the Himalayas. Here is a picture of myself, taken after a small mountain hike.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*EVZL88BNmc_mAz_ViXDw4g.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;I love to code, and I love Astrophysics. There had been multiple moments in my life when I had to choose between Physics and Computer Science, and every time I wished that I can choose both. For my research work, I extensively use open-source packages like Astropy, CASA, etc. This motivated me to contribute to open-source packages for astronomy and give something back to the community. Surprisingly, this year GSoC also relaxed its eligibility criteria for contributors. All of these have brought me this serendipitous point in my life when I have an opportunity to start my journey in open-source, writing code for software used for Astrophysics while participating in a prestigious program like GSoC! So this is like a dream come true for me.&lt;/p&gt;
&lt;p&gt;I think I will wrap up this blog for now! I plan to write about my project ‘Bayesian Excess Variance (Bexvar) in Stingray’ soon in this series. If you want to know a little bit about it now, you can take a look at this &lt;a href="https://summerofcode.withgoogle.com/programs/2022/projects/rG1XqJqK"&gt;webpage&lt;/a&gt;. Till then goodbye :)!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=bb7ba9c5026c" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2022/06/20220616_0650_mihirtripathi97/</guid><pubDate>Thu, 16 Jun 2022 05:50:44 GMT</pubDate></item><item><title>GSoC Blog #0</title><link>http://openastronomy.org/Universe_OA/posts/2022/06/20220614_1714_aman-pandey-afk/</link><dc:creator>AMAN PANDEY</dc:creator><description>&lt;p&gt;With the end of the community bonding period, my GSoC 2022 project has officially started. It will be an exciting journey, and I will be documenting my experience and work in a series of blogs in the future.&lt;/p&gt;
&lt;p&gt;I’m &lt;strong&gt;Aman Pandey, &lt;/strong&gt;and this blog is about my introduction to open source and learning Julia that lead me to take part in the Google Summer of Code for Open Astronomy, juliaAstro and Stingray to be specific. I also enlist my community bonding period work and what I intend to do next week.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/1*dfjiDfMcY8h9eEOySAnxbA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;My journey to Open Source&lt;/strong&gt;&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/310/1*6cUCFKPtaiyl4vQ3Q1YkLA.jpeg"&gt;&lt;/figure&gt;&lt;p&gt;My intro to the open-source world was through a game called TwilioQuest. It had a level where I had to configure git and GitHub and then contribute to their Open Pixel Art repo by changing a JSON file. It was fun, but I wasn’t fully aware of the open-source community until I started understanding the Godot engine ( A 3D game engine in C++). I followed the instructions for compiling it on Windows using VSCode, but I met an issue. I couldn’t reproduce the instructions of the doc and created a pull request to change the docs when I found the solution. The community was very helpful, and after some suggestions, I could finally merge my first PR!&lt;/p&gt;
&lt;p&gt;Through the internet, I learned about GSoC as a program to foster an open-source culture among students, especially to guide newcomers to contribute code and engage with the community. I started searching for organizations that interested me and found OpenAstronomy (I was fascinated by the different projects going on in the organization). A thorough search made me sure to start learning Julia to contribute to the “Spectral Timing in Julia” project. While trying to contribute, I was fascinated by the structure of a Julia program and its excellent features like multiple dispatch and Abstract and Concrete Type System. Part of it was due to the incredible guidance by one of the project’s mentors, &lt;strong&gt;Mosè Giordano, &lt;/strong&gt;which shows the role of community while developing open-source software. I was thrilled when my project was announced as selected; it will be a great summer this year!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Community Bonding Period&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;My project involves porting the Stingray package of python to Julia, which consists of different time series analysis methods to deal with periodicities in X-Ray signals coming from massive celestial objects. I started the period by learning about different methods implemented in the python package like creating periodograms, normalizing them, and using them for data types like NumPy iterables or custom objects like LightCurves and EventLists. At the same time, I focused on learning about Julia and its best practices and understanding how to write clean, high-performance, and documented code.&lt;/p&gt;
&lt;p&gt;I committed my first PR for the project by initializing the package and adding documentation and continuous integration (CI) support. I then looked forward to porting the fourier.py file and implementing tests alongside it and was reasonably successful in it. In my opinion, it was a significant period; I discussed many exciting things with the mentors, like using DataFrames and generating distribution to test the periodograms with array inputs. I myself worked on writing type-stable code (which I was introduced to while working on a PR for another juliaAstro package, AstroLib) and diving deep into topics like dispatch and type systems, which are the testament that open source contribution can enhance your developer skills multi-folds as these topics are not restricted to any specific programming language.&lt;/p&gt;
&lt;p&gt;An example of type-unstable function :&lt;/p&gt;
&lt;pre&gt;function foo()&lt;br&gt;    x = 1&lt;br&gt;    for i = 1:10&lt;br&gt;        x /= rand()&lt;br&gt;    end&lt;br&gt;    return x&lt;br&gt;end&lt;/pre&gt;&lt;p&gt;&lt;strong&gt;Upcoming Week&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I intend to implement the utility functions and fourier.py file of Stingray in Julia to create periodograms and cross spectra from tables. Then, I will move on to implement the light curves and event lists, through which, in the future, I will be able to create full-fledged periodograms and cross-spectra. Proper Testing and performance-related aspects will be sincerely followed throughout the project.&lt;/p&gt;
&lt;p&gt;I am excited about this summer and expect to learn much about software development, open-source, and the programming community throughout this journey. Thanks to Google, Open Astronomy, juliaAstro, Stingray, and my mentors &lt;a href="https://github.com/matteobachetti"&gt;Matteo Bachetti&lt;/a&gt; and &lt;a href="https://github.com/giordano"&gt;Mosè Giordano&lt;/a&gt; for providing me this opportunity. Let the coding phase start!&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=8f65bf844cd8" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2022/06/20220614_1714_aman-pandey-afk/</guid><pubDate>Tue, 14 Jun 2022 16:14:10 GMT</pubDate></item><item><title>Google Summer of Code- Final Evaluation</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1027_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;h4&gt;&lt;strong&gt;Topic: Implement JAX based automatic differentiation to Stingray&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The project involved the study of modern statistical modelling to augment the accuracy, speed, and robustness of the likelihood function, into a software package called Stingray. This report demonstrates the experiment done for a combination of different optimizers to fit the scipy.optimize function. Another emphasis is to investigate the gradient calculation using JAX and compare it with scipy.optimize.&lt;/p&gt;
&lt;h4&gt;&lt;strong&gt;Introduction:&lt;/strong&gt;&lt;/h4&gt;&lt;p&gt;The proposed milestone was to investigate the room for improvement to enhance the overall performance of modelling to Stingray, using JAX. However, the current stage of the model is still a sandbox model. Stingray is astrophysical spectral timing software, a library in python built to perform time series analysis and related tasks on astronomical light curves. JAX is a python library designed for high-performance numerical computing. Its API for numerical functions is based on NumPy, a collection of functions used in scientific computing. Both Python and NumPy are widely used and familiar, making JAX simple, flexible, and easy to adopt. It can differentiate through a large subset of python’s features, including loops, ifs, recursion, and closures, and it can even take derivatives of derivatives. Such modern differentiation packages deploy a broad range of computational techniques to improve applicability, run time, and memory management.&lt;/p&gt;
&lt;p&gt;JAX utilizes the grad function transformation to convert a function into a function that returns the original function’s gradient, just like Autograd. Beyond that, JAX offers a function transformation jit for just-in-time compilation of existing functions and vmap and pmap for vectorization and parallelization, respectively.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h4&gt;Experiment:&lt;/h4&gt;&lt;p&gt;The powerlaw and lorentzian function are the most used to describe periodograms in astronomy. In practice, we use the sum of these components to design a realistic model. For the analysis here we consider a quasi-periodic oscillation and a constant and try to fail the algorithm by, (i) reduce the amplitude, (ii) start the optimization process with parameters very far away from the true parameters, (iii) try different optimizers to experiment on different sensitive aspect of the current likelihood calculation. The current ongoing milestone is to try alternatives of scipy.optimize but this requires series of tests for the same.&lt;/p&gt;
&lt;p&gt;The above tests can be visualized in the notebook added on Github: &lt;a href="https://github.com/rashmiraj137/GSoC-Project"&gt;https://github.com/rashmiraj137/GSoC-Project&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Results:&lt;/h4&gt;&lt;p&gt;During the experiment, it was observed that the algorithm is sensitive to input parameters i.e., it fails for couple of combinations like when the amplitude is set far away from true value, precisely blow absolute value of 1. In general, if we set the parameters very far away from the true value, it fails to approximate the likelihood function. In the notebook (&lt;a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb"&gt;link&lt;/a&gt;), we demonstrate the room for improvement in the current algorithm by choosing a different set of parameters. The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy was to find alternatives that potentially accelerate the code, makes it robust.&lt;/p&gt;
&lt;p&gt;For this experiment case, we choose a couple of optimizers and compare the robustness to Powell (the current optimizer used). So we visualize the fit for couple of optimizers like :&lt;/p&gt;
&lt;ul&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-neldermead.html"&gt;minimize(method=’Nelder-Mead’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-powell.html"&gt;minimize(method=’Powell’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cg.html"&gt;minimize(method=’CG’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html"&gt;minimize(method=’BFGS’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-newtoncg.html"&gt;minimize(method=’Newton-CG’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-lbfgsb.html"&gt;minimize(method=’L-BFGS-B’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-tnc.html"&gt;minimize(method=’TNC’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-cobyla.html"&gt;minimize(method=’COBYLA’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;&lt;a href="https://docs.scipy.org/doc/scipy/reference/optimize.minimize-slsqp.html"&gt;minimize(method=’SLSQP’)&lt;/a&gt;&lt;/li&gt;&lt;li&gt;minimize(method = “trust-constr”)&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;This notebook (&lt;a href="https://github.com/rashmiraj137/GSoC-Project/blob/main/GSoC_Evaluation%20Notebook.ipynb"&gt;link&lt;/a&gt;) has results using each method and it was observed that Nelder-Mead is more robust as compared to other optimizers. Another optimizer like dogleg, trust-ncg, might be good as well, but the jacobian and hess need to be calculated for them.&lt;/p&gt;
&lt;h4&gt;Future Work:&lt;/h4&gt;&lt;p&gt;JAX has now its own version of scipy.optimize.minimize but it has couple of bugs and is not as robust as scipy.optimize.minimize. Finding an alternative for scipy.optimize.minimize such that it doesn’t fails even if the start parameters are far away from the true parameters was a goal for this project but unfortunately JAX did not assist that well enough. But there might be a superior algorithm to scipy.optimize.minimize that can be useful.&lt;/p&gt;
&lt;h4&gt;Repositories:&lt;/h4&gt;&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/stingray"&gt;https://github.com/StingraySoftware/stingray&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/StingraySoftware/notebooks"&gt;GitHub - StingraySoftware/notebooks: Tutorial notebooks for Stingray&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Blog Post:&lt;/h4&gt;&lt;p&gt;1. &lt;a href="https://raj-rashmi741.medium.com/jax-based-automatic-differentiation-introduction-of-modern-statistical-modeling-to-stingray-1bc26da7571f"&gt;JAX-based automatic differentiation: Introduction of modern statistical modeling to Stingray.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;2. Insight of Implementation of JAX to stingray- GSoC coding period!&lt;/p&gt;
&lt;p&gt;3. &lt;a href="https://raj-rashmi741.medium.com/gsoc-update-2d16a70cc267?source=your_stories_page-------------------------------------"&gt;GSoC update!&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href="https://raj-rashmi741.medium.com/time-to-review-my-gsoc-project-c34297f2dc81"&gt;4. Time to review my GSoC Project.&lt;/a&gt;&lt;/p&gt;
&lt;h4&gt;Profiles:&lt;/h4&gt;&lt;p&gt;GitHub: &lt;a href="https://github.com/rashmiraj137"&gt;https://github.com/rashmiraj137&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;LinkedIn: &lt;a href="https://www.linkedin.com/in/rashmi-raj-4b8a2b106/"&gt;https://www.linkedin.com/in/rashmi-raj-4b8a2b106/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Medium: &lt;a href="https://raj-rashmi741.medium.com/"&gt;https://raj-rashmi741.medium.com/&lt;/a&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=578c0088bcbd" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210823_1027_rashmiraj137/</guid><pubDate>Mon, 23 Aug 2021 09:27:57 GMT</pubDate></item><item><title>Time to review my GSoC Project</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210816_1603_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;p&gt;With the end of the GSoC project, I will give this blog to summarise the JAX based optimization to analyze its applicability to enhance the loglikelihood calculation. The goal is to analyze, (i) the performance of different optimizers to evaluate the loglikelihood function, (ii) demonstrated the robustness of JAX to calculate gradients. And talk about the current code and corresponding improvement due to JAX.&lt;/p&gt;
&lt;p&gt;The application of loglikelihood fitting to periodograms is discussed in [1]. Let us start with analyzing best-fit power spectrum (i) with different sets of optimizers namely: &lt;em&gt;minimize(method=’Nelder-Mead’, ’Powell’, ’CG’, ’BFGS’, ’Newton-CG’, ’L-BFGS-B’, ’TNC’, ’COBYLA’, ’SLSQP’, ’trust-constr’, ’dogleg’, ’trust-ncg’, ’trust-krylov’, ’trust-exact’). &lt;/em&gt;The problem setting shifts the start and test parameters to study the graph of best fit optimizer using different “methods” listed above. First, we will stick with the Powell optimizer and try to check what is the current sensitivity of the implementation.&lt;/p&gt;
&lt;p&gt;Currently, we seek to find a solution to the problem when the optimization algorithm often gets stuck in local minima, terminate without meeting its formal success criteria, or fails due to any contributing factor. Possible ways are: (1) add more Lorentzian components, (2) reduce the amplitude, (3) start the optimization process with parameters very far away from the true parameters, (4) experiment with the different optimizers/ “methods” to investigate if there is more superior algorithm compared to Powell.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*oEfdtoL2Fa0XAbjmugMvnA.png"&gt;&lt;figcaption&gt;Reference: blog.gitguardian.com&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;So far the &lt;em&gt;Powell&lt;/em&gt; and &lt;em&gt;Nelder-Mead &lt;/em&gt;gives almost the same best-fit curve compared to other optimizers, surprisingly even better than &lt;em&gt;BFGS(which is a well-known &lt;/em&gt;numerical optimizer for an iterative method for solving unconstrained nonlinear optimization problems. This directs to more investigation with (1) and (2) and (3). Both (2) and (3) makes the algorithm fail with the current &lt;em&gt;scipy.optimize.minimize, &lt;/em&gt;and we can see the&lt;em&gt; &lt;/em&gt;graph as given below.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/1*1SA0BwGc48I8qRozxuK6CQ.png"&gt;&lt;/figure&gt;&lt;p&gt;I am still holding on to try &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html"&gt;jax.scipy.optimize.minimize&lt;/a&gt; instead of &lt;a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.scipy.optimize.minimize.html"&gt;scipy.optimize.minimize&lt;/a&gt; and analyze the increment in robustness. Another way to enhance the current algorithm alongside experimenting with different optimisers is:&lt;/p&gt;
&lt;ol&gt;&lt;li&gt;Use a different gradient finding method.&lt;/li&gt;&lt;li&gt;Speed up objective function.&lt;/li&gt;&lt;li&gt;Reduce the number of design variables.&lt;/li&gt;&lt;li&gt;Choose a better initial guess.&lt;/li&gt;&lt;li&gt;Use parallel processing.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;In my next blog, I will provide a more detailed explanation of current events. In this blog, I highlighted the emphasis of analysis.&lt;/p&gt;
&lt;p&gt;References:&lt;/p&gt;
&lt;p&gt;[1] Maximum likelihood fitting of X-ray power density spectra: Application to high-frequency quasi-periodic oscillations from the neutron star X-ray binary 4U1608-522. Didier Barret, Simon Vaughan. &lt;a href="https://arxiv.org/abs/1112.0535"&gt;https://arxiv.org/abs/1112.0535&lt;/a&gt;&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=c34297f2dc81" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210816_1603_rashmiraj137/</guid><pubDate>Mon, 16 Aug 2021 15:03:25 GMT</pubDate></item><item><title>A Glimpse into my GSoC project</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210812_2028_dhruv9vats/</link><dc:creator>Dhruv Vats</dc:creator><description>&lt;p&gt;While all my blog posts till now were kind of abstract, here I will try to show some of the technical details of the project without making it too bloated. So as a one-line description, I had to study, implement and integrate a spectral estimation technique, namely the &lt;em&gt;Multitaper Periodogram³&lt;/em&gt; (and its derivatives¹), which are used to analyze astronomical time series.&lt;/p&gt;
&lt;h5&gt;Why spectral representations?&lt;/h5&gt;&lt;p&gt;Before getting into the how of spectral analysis and its estimation, a brief sidenote on the why. Why do we even bother to study the spectral properties of a time series? It turns out, some of the determining characteristics or defining parameters associated with a certain time series are better &lt;em&gt;brought out&lt;/em&gt; in their spectral representations (frequency domain representations).&lt;/p&gt;
&lt;p&gt;As an example, the power spectral density is a common tool to try and unearth the periodic element(s) in a time series. Such spectral analysis techniques, at their core, are enabled by the Fourier Transform, and if you’d like to gain a better intuitive understanding of it, do check out &lt;a href="https://www.youtube.com/watch?v=spUNpyF58BY"&gt;this awesome video&lt;/a&gt; by &lt;a href="https://www.youtube.com/c/GrantSanderson"&gt;Grant Sanderson&lt;/a&gt; on &lt;a href="https://www.youtube.com/channel/UCYO_jab_esuFRV4b17AJtAw"&gt;3Blue1Brown&lt;/a&gt;.&lt;/p&gt;
&lt;!-- TEASER_END --&gt;
&lt;h5&gt;Multi what?&lt;/h5&gt;&lt;p&gt;Spectral analysis isn’t without its fair share of practical complications, in fact, far from it. But there have been quite a few very effective techniques to mitigate them.&lt;/p&gt;
&lt;p&gt;One of them is &lt;em&gt;tapering (&lt;/em&gt;multiplying the time series with a bell-shaped function), effective in reducing spectral leakage. Tapering a time series as a way of obtaining a spectral estimator with acceptable bias properties is an important concept. The loss of information (contained at the extremes of the time series) inherent in tapering can often be avoided either by prewhitening or by using Welch’s overlapped segment averaging.&lt;/p&gt;
&lt;p&gt;The multitaper periodogram is another approach to recover information lost due to tapering. This approach was introduced by Thomson (1982)³ and involves the use of multiple orthogonal tapers, having approximately uncorrelated spectral densities.&lt;/p&gt;
&lt;p&gt;In the multitaper method, the data is windowed or tapered, but this method differs from the traditional methods in the tapers used, which are the most band-limited functions amongst those defined on a finite time domain, and also, these tapers are orthogonal, enabling us to average the &lt;em&gt;eigenspectrum&lt;/em&gt; (spectrum estimates from individual tapers) from more than one tapers to obtain a superior estimate in terms of noise. The resulting spectrum has low leakage, low variance, and retains information contained in the beginning and end of the time series.&lt;/p&gt;
&lt;p&gt;The tapers used are the discrete prolate spheroidal sequences (DPSS), or, the Slepians (Slepian 1978)⁴.&lt;/p&gt;
&lt;h5&gt;A look at the DPSS tapers&lt;/h5&gt;&lt;p&gt;Let’s consider a time series sampled from an autoregressive process of order 4, AR(4), which has been frequently exemplified in literature¹ in similar contexts.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/657/1*h-k9Xm1CgerMQVIeKbfyOw.png"&gt;&lt;figcaption&gt;A time series sampled from an autoregressive process of order 4.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;A good way to gain an intuitive understanding of the properties of the DPSS tapers, and how they affect the time series, is to visualize the effect. Given here are the time and frequency domain representations of the tapers and the tapered time series.&lt;/p&gt;
&lt;p&gt;The first 8 tapers and the corresponding tapered time series⁵.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/645/1*SEG0OEstBuAgP5OqgnS4zA.png"&gt;&lt;/figure&gt;&lt;p&gt;This showcases the product of a windowing function and a time series quite well. Next let’s have a look at their spectral representations⁵, more specifically, their power spectrum densities (PSD).&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/680/1*nvqbaD08gDf1zdh9P2ljqQ.png"&gt;&lt;figcaption&gt;For this example, we took the normalized half-bandwidth product to be equal to 4 (NW = 4), resulting in 8 tapers being used. The spectral concentration in the band [-W, W] can then be seen from the plots. (Here N, the number of data points, is 1024, hence, W = 4/N = 0.003906)&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;There is a significant increase in the bias of the PSD estimates as the spectral concentration of the tapers worsens. To prevent these estimates with greater biases from affecting the final averaged estimate but still use the variance reductions they bring, we weigh the different estimates according to their spectral concentration (percentage of energy concentrated in the desired frequency band).&lt;/p&gt;
&lt;p&gt;This can be kicked up a notch by using what is called adaptive weighing, which adaptively (duh?!) combines the different estimates, calculating the weights using an iterative process.&lt;/p&gt;
&lt;h5&gt;A brief summary of the Multitaper spectral estimation&lt;/h5&gt;&lt;a href="https://medium.com/media/3f23eb5c2c41460b8793fbc2e6fbc04d/href"&gt;https://medium.com/media/3f23eb5c2c41460b8793fbc2e6fbc04d/href&lt;/a&gt;&lt;p&gt;This summary, by no means, is an exhaustive explanation of the multitapering concept. Further exploration of the topic is highly encouraged. Use the references as the starting point.&lt;/p&gt;
&lt;h5&gt;The Final Result&lt;/h5&gt;&lt;p&gt;Using all the techniques outlined here, let's see how well can this multitaper periodogram estimate the true spectrum of this auto-regressive process. Also added is the classical periodogram (also sometimes referred to as a naïve spectrum estimator because of its basic estimation process) for comparison.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/707/1*7eYiabftab1cYuMpBC4uqA.png"&gt;&lt;figcaption&gt;Here the multitaper estimate uses the adaptive weighting technique and the first 7 DPSS tapers&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;All this functionality is now implemented in &lt;a href="https://github.com/StingraySoftware/stingray"&gt;Stingray&lt;/a&gt;²&lt;/p&gt;
&lt;h5&gt;References&lt;/h5&gt;&lt;p&gt;[1]: Springford, Aaron, Gwendolyn M. Eadie, and David J. Thomson. 2020. “Improving the Lomb–Scargle Periodogram with the Thomson Multitaper.” The Astronomical Journal (American Astronomical Society) 159: 205. doi:10.3847/1538–3881/ab7fa1.&lt;/p&gt;
&lt;p&gt;[2]: Huppenkothen, Daniela, Matteo Bachetti, Abigail L. Stevens, Simone Migliari, Paul Balm, Omar Hammad, Usman Mahmood Khan, et al. 2019. “Stingray: A Modern Python Library for Spectral Timing.” The Astrophysical Journal (American Astronomical Society) 881: 39. doi:10.3847/1538–4357/ab258d.&lt;/p&gt;
&lt;p&gt;[3]: Thomson, D. J. 1982. “Spectrum Estimation and Harmonic Analysis.” IEEE Proceedings 70: 1055–1096. &lt;a href="https://ui.adsabs.harvard.edu/abs/1982IEEEP..70.1055T."&gt;https://ui.adsabs.harvard.edu/abs/1982IEEEP..70.1055T.&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;[4]: Slepian, D. 1978. “Prolate Spheroidal Wave Functions, Fourier Analysis, and Uncertainty-V: The Discrete Case.” Bell System Technical Journal (Institute of Electrical and Electronics Engineers (IEEE)) 57: 1371–1430. doi:10.1002/j.1538–7305.1978.tb02104.x&lt;/p&gt;
&lt;p&gt;[5]: D.B. Percival and A.T. Walden, Spectral Analysis for Physical Applications: Multitaper and Conventional Univariate Techniques. Cambridge, U.K.: Cambridge Univ. Press, 1993.&lt;/p&gt;
&lt;p&gt;[6]: Thomson, D. J. 1990. “Time series analysis of Holocene climate data.” Philosophical Transactions of the Royal Society of London. Series A, Mathematical and Physical Sciences (The Royal Society) 330: 601–616. doi:10.1098/rsta.1990.0041&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=25c0fe3296dd" width="1"&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210812_2028_dhruv9vats/</guid><pubDate>Thu, 12 Aug 2021 19:28:01 GMT</pubDate></item><item><title>GSoC update!</title><link>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</link><dc:creator>Raj Rashmi</dc:creator><description>&lt;p&gt;GSoC started four months ago and it is not just about knowing more about the open-source that made the experience great! My mentors made it way cooler than I thought it would be. I was writing my Master thesis, for the last three months and surely, it has been a super productive summer for me! The best part is I get to do things at my own pace. My project particularly hasn’t been very easy to implement. I need to bridge a Machine Learning algorithm in the existing codebase. The fun part is venturing with different notebooks and figuring out with intuition, what could be efficient in terms of computational time, efficiency, cost etc. But as of now, the struggle has been to define the problem as exactly to achieve the result. But I will keep working on finding a solution with my mentor Daniela, and trust that struggle will bring some positive construction in Stingray.&lt;/p&gt;
&lt;figure&gt;&lt;img alt="" src="https://cdn-images-1.medium.com/max/1024/0*2GcUC2cgvKapk8Lj"&gt;&lt;/figure&gt;&lt;p&gt;The current data fit for the evaluation of likelihood happens using scipy.optimize.minimize function. However, there exists numerous ways to do this. SciPy optimize provides functions for minimizing (or maximizing) objective functions, possibly subject to constraints. It includes solvers for nonlinear problems (with support for both local and global optimization algorithms), linear programming, constrained and nonlinear least-squares, root finding, and curve fitting. The problem with the current minimization algorithm is that it converges at local minimum instead of global, i.e. it is not very robust. Recently, Machine Learning has evident development in such optimization tools. The strategy for ahead is that I will work on finding alternatives that potentially accelerate the code, makes it robust.&lt;/p&gt;
&lt;img alt="" height="1" src="https://medium.com/_/stat?event=post.clientViewed&amp;amp;referrerSource=full_rss&amp;amp;postId=2d16a70cc267" width="1"&gt;
&lt;!-- TEASER_END --&gt;</description><category>stingray</category><guid>http://openastronomy.org/Universe_OA/posts/2021/08/20210803_0200_rashmiraj137/</guid><pubDate>Tue, 03 Aug 2021 01:00:58 GMT</pubDate></item></channel></rss>